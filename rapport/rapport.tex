\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Report for Project 10 ``Multi-armed bandit and zero-sum games (theory)''}


\author{
Leman FENG \\
Ecole des Ponts ParisTech\\
\texttt{flm8620@gmail.com} \\
Website: \texttt{lemanfeng.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\section{Normal Form Games}
In a normal K-person game, Player $k\in \{1,\dots,K\}$ has $N_k$ actions to choose from. All actions made from all players is a K-tuple $\mathbf{i} = (i_1,\dots,i_k)$. Each component $i_k\in\{1,\dots,N_k\}$. Every player choose his actions without knowing the choice of other players. Once all players have chosen their action, the result of game is given, which is the loss $l^{(k)}(\mathbf{i})\in[0,1]$ suffered by player $k$.

The game is played once and then ends. The goal of each player is to minimize his loss.

A player can use a randomized strategy to choose action. A mixed strategy for one player is a probability distribution $\mathbf{p}^{(k)}$ over its all possible actions. When he plays the game, an action is randomly choose according to this distribution. More formally, this randomization can be seen generated from a uniform random variable taking value from $[0,1]$. 

If all players use mixed strategies, and the randomization between players is independent, then the joint distribution $\pi$ of K-tuple actions played by all players following the distribution defined by product of all mixed strategies $\{\mathbf{p}^{(j)}\}_k$

\section{Nash Equilibrium \& Correlated Equilibrium}
The joint distribution $\pi$ of K-tuple actions, made from product of mixed strategies $\{\mathbf{p}^{(j)}\}_k$, is a Nash equilibrium, in the sense that, if all players know the mixed strategies of others, then each one of them has no intention to modify his mixed strategy, because any modification of his strategy will lead to a higher expected loss for him.

Nash proved that at least one Nash equilibrium exist for every finite game.

If the joint distribution $\pi$ is not a product distribution, which means the above randomization between players is not independent, then a generalized notion of Nash Equilibrium can be introduced, called correlated equilibrium. In a correlated equilibrium, for each player, if he knows all other players will follow the joint distribution $\pi$, then he has no intention to modify his.

\section{Two-person Zero-sum Games}
A two-person zero-sum game is a normal form game where only played by two players, and the loss of two players always sums to zero. If two players use mixed strategies $\mathbf{p}$ and $\mathbf{q}$, then it's shown that if the pair $\mathbf{p},\mathbf{q}$ is a Nash equilibrium if and only if
$$
\min_{\mathbf{p}} \max_{\mathbf{q}} l(\mathbf{p},\mathbf{q}) = \max_{\mathbf{q}} \min_{\mathbf{p}} l(\mathbf{p},\mathbf{q})
$$
And the value of this equation $V$ is called the value of the game.

I think it's easy to find a straight forward interpretation for this equation. $\max_{\mathbf{q}} l(\mathbf{p},\mathbf{q})$ represent that if the second player knows the first player's strategy $\mathbf{p}$, then the second player will choose the best mixed strategy $\mathbf{q}' = \arg\max_{\mathbf{q}}l(\mathbf{p},\mathbf{q})$ to increase first player's loss. Knowing the fact above, the first player should choose his best strategy $\mathbf{p}' = \arg\min_{\mathbf{p}}\max_{\mathbf{q}}l(\mathbf{p},\mathbf{q})$ to fight against the second player and decrease his loss in the worst case. And by the symmetric of the zero-sum game, this is the same for the second player.

If the two players use this reasoning, and they choose their mixed strategies $\mathbf{p}',\mathbf{q}'$, then after their mixed strategies have been revealed to each other, then no one will regret choosing his strategy.

\section{Uncoupled Repeated Games}
Consider a normal K-person game which is played repeatedly. At round $t$, Each player choose mixed strategy $\mathbf{p}^{(k)}_t$. The randomization of all players is independent between players and between games played.

Then we limit ourself in a "uncoupled" way of playing, where each player only knows and only cares his own loss function $l^{(k)}$. And he only choose he next strategy according to his past losses. 

Personnaly I think the definition of "uncoupled" way in this book is very unclear. By reading carefully the following sections, I think it's important to notice that, the player $k$ know not only his past losses, but also his loss function $l^{(k)}(\mathbf{I})$, for any the K-tuple actions $\mathbf{I}=(I^{(1)},\dots,I^{(k)},\dots,I^{(K)})$. At round $t$, the player $k$ should also know all other players' history actions $\mathbf{I}_1,\dots,\mathbf{I}_{t-1}$, because otherwise knowing his loss function $l^{(k)}(\mathbf{I})$ is useless.

But we just said player $k$ don't know other's loss function and only cares his own losses, one may ask why player $k$ should care about other's history actions, and why not just consider the game as a total black box for player $k$. Well, one should notice that an important usage of the loss function $l^{(k)}(\mathbf{I})$ combined with all history of players' actions is that player $k$ can know, after the round $t$, the loss value $l^{(k)}(I^1_t,\dots,{I^{(k)}_t}',\dots, I^{(K)}_t)$ which represent the loss he would have suffered at round $t$ if he had played ${I^{(k)}_t}'$ instead of $I^{(k)}_t$ at that time.

But the reason why we don't let player $k$ know others' loss functions is that we don't allow player $k$ to know why other players made their moves. Player $k$ should not analyse the motivation nor any pattern of other players' move.

Thus, with all these details above, I think it's better to reformulate the "uncoupled" playing like this: 

Player $k$ doesn't need to realize the existence of all other players. For player $k$, the game is like an environment that can change it state $s\in\{1,\dots,N_s\}$ over time. After taking one action $I_t$ at time $t$, the environment will not only give the loss $l(I_t, s_t)$, but also the state $s_t$. Then the table of all losses $l(i, s_t), i\in\{1,\dots,N_k\}$ if player $k$ had played actions $i$ can be known by player. Then player $k$ can decide his action for next round $t+1$ bases on all his history actions and all these history tables $l(i, s_\tau), i\in\{1,\dots,N_k\}, \tau\in\{1,\dots,t\}$.

So it's equivalent to have only two players or many players, since all other players can be seen as one environment and the number of states of the environment is simply the product of action numbers $N_{k'}$ for all other players $k'\neq k$.

By this reformulation, we can see very clearly that for each player $k$, the "uncoupled" way of playing really limit the information he can use. And we can understand more easily why in "uncoupled" way of playing, the author called this a regret-based procedure. Because what the player can do is only playing an action, observing the loss table for all possible actions, then knowing the regret that he could have done better if he played another action, and rethinking for the next round.

The "uncoupled" way made the problem simple, thus many results from precedent chapters can be applied here. But the interesting part of this chapter is that, by this very limited "uncoupled" way, there are still some equilibria can be reached in game.

\section{Hannan Consistent Strategy}
I want to review the Hannan consistency although this is a term defined in previous chapter in the book. We should firstly review the notion of external and internal regret.

If we consider other players' move as the state of environment, we note $s_t$ the state of environment at time $t$, and note $\mathbf{p}_t$ as the strategy of player $k$ at time $t$. Then the internal regret of player $k$ is defined as:

$$
\sum_{t=1}^n l(\mathbf{p}_t, s_t) - \min_{i=1,\dots,N_k} \sum_{t=1}^n l(i, s_t)
$$

Intuitively, this is the regret of player $k$ of his cumulative loss from time $t=1$ to $n$ compared to the cumulative loss of a constant strategy $i$ played against the same history state $\{s_t\}_{t=1\dots n}$. Notice that the latter is different from the cumulative loss obtained by a playing of constant strategy because once player $k$'s components find out that he play constant move, they might learn this and fight back easily.

And the internal regret is defined as:

$$
\max_{i,j=1,\dots,n}\sum_{t=1}^n p_{i,t}  (l(i,s_t)-l(j,s_t))
$$

which can be explained easily as the regret of player $k$ if he had chosen action $j$ each time he chose $i$. It's shown in chapter 4 that a small internal regret implies a small external regret, but not in the reverse. 

Then it's time to introduce the Hannan consistency. We say the player $k$ has a Hannan consistency strategy if
\begin{equation}
\limsup_{n\to \infty} \frac{1}{n}(l(I_t, s_t) - \min_{i=1,\dots,N_k}\sum_{t=1}^n l(i, s_t)) = 0\
\end{equation}
with probability 1.

Hannan consistency basically means the amortised external regret for player $k$ is zero. It's shown that Hannan consistency is achievable if the actions space and environment state space is finite. And one example is a simple exponential strategy:
$$
p_i,t = \frac{\exp{ ( -\eta \sum_{s=1}^t ) l(i,s_t) } }{ \sum_{k=1}^{N_k} \exp{(-\eta \sum_{s=1}^t l(k,s_t) )} }
$$
It can be interpreted as player $k$ should choose the best action $i$, ``best'' in the sense that if player $k$ should replace all his previous actions by one constant action, then he should choose $i$. Then to make the strategy more random, it's better to take a softmax function and play a probability distribution.

I think it is not hard to imagine why Hannan consistency is always possible. For a finite game, when you played it long enough, you will have found out the frequency of environment state $s$. Since you know your loss function for all combination of your action and environment state, you can play according to the empirical probability distribution of environment state. But you should not make a naive strategy seen through easily by opponents, so the best way to hide your pattern is to randomize your strategy with a reasonable probability distribution.

But there is another corollary that states, there is always a loss function $l$ such that any randomized strategy of player $k$ will suffer at least an amortized regret of order $\frac{1}{\sqrt{n}}$.

\section{Link to Multi-Armed Bandit}
The stochastic Multi-Armed Bandit we saw in class is different from the above model. But I found later in the slides that the above model is called Adversarial Multi-Armed Bandit, or called Non-Stochastic Multi-Armed Bandit. I think we should use both Adversarial and Non-Stochastic to describe it. 

To change from a stochastic MAB to Non-Stochastic MAV, we drop the probability distribution of each arm, so each arm will give only a constant reward. Suddenly the game is so easy. You just need to pull each arm to know the whole information. To make it harder, we let the environment have right to change its state at each round. Each arm will give a deterministic reward according to current state. But the player won't know the current state until he made an action.

So the uncertainty for player changed from the schochastic nature of arm to an unpredictable environment, in exchange, the player can know the loss/reward for all pairs of arm and states.

The Adversarial MAB is much harder than Stochastic MAV. We can see this from the bound on regret in each case. For Stochastic MAV, the upper and lower bound on amortised regret are both in ordre $\frac{\log{n}}{n}$. But for Adversarial MAV, we can only say Hannan consistency strategy can make amortised regret converge to zero, but there is a lower bound $\frac{1}{\sqrt{n}}$, which is way larger than the upper bound in Stochastic MAB. So we can say the prediction of environment's move is much harder than trying a set of machine which don't change their internal parameter over time.

\end{document}
